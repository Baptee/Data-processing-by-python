# -*- coding: utf-8 -*-
"""22141041 Lab 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yJyCZ1WkF0nAM2N3QIsxP6CoQkL_dQsZ
"""

#importing necessary libraries
import pandas as pd
import numpy as np

"""Load the dataset as a pandas dataframe"""

volunteer = pd.read_csv('/content/Income Dataset (50k).csv')
volunteer.head(50)

volunteer.shape

volunteer.isnull().sum()

from sklearn.impute import SimpleImputer

impute = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

impute.fit(volunteer[['workclass']])

volunteer['workclass'] = impute.transform(volunteer[['workclass']])


impute = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

impute.fit(volunteer[['occupation']])

volunteer['occupation'] = impute.transform(volunteer[['occupation']])


impute = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

impute.fit(volunteer[['native-country']])

volunteer['native-country'] = impute.transform(volunteer[['native-country']])



impute = SimpleImputer(missing_values=np.nan, strategy='most_frequent')

impute.fit(volunteer[['gender']])

volunteer['gender'] = impute.transform(volunteer[['gender']])



impute = SimpleImputer(missing_values=np.nan, strategy='mean')

impute.fit(volunteer[['capital-gain']])

volunteer['capital-gain'] = impute.transform(volunteer[['capital-gain']])



impute = SimpleImputer(missing_values=np.nan, strategy='mean')
impute.fit(volunteer[['capital-loss']])

volunteer['capital-loss'] = impute.transform(volunteer[['capital-loss']])


impute = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
impute.fit(volunteer[['hours-per-week']])

volunteer['hours-per-week'] = impute.transform(volunteer[['hours-per-week']])



impute = SimpleImputer(missing_values=np.nan, strategy='mean')
impute.fit(volunteer[['income_>50K']])

volunteer['income_>50K'] = impute.transform(volunteer[['income_>50K']])

volunteer.isnull().sum()

volunteer.info()

# Transform the category_desc column
category_enc = pd.get_dummies(volunteer['workclass'])

# Take a look at the encoded columns
category_enc.head()

# Transform the category_desc column
category_enc = pd.get_dummies(volunteer['education'])

# Take a look at the encoded columns
category_enc.head()

# Transform the category_desc column
category_enc = pd.get_dummies(volunteer['marital-status'])

# Take a look at the encoded columns
category_enc.head()

# Transform the category_desc column
category_enc = pd.get_dummies(volunteer['occupation'])

# Take a look at the encoded columns
category_enc.head()

# Transform the category_desc column
category_enc = pd.get_dummies(volunteer['relationship'])

# Take a look at the encoded columns
category_enc.head()

# Transform the category_desc column
category_enc = pd.get_dummies(volunteer['race'])

# Take a look at the encoded columns
category_enc.head()

# Transform the category_desc column
category_enc = pd.get_dummies(volunteer['gender'])

# Take a look at the encoded columns
category_enc.head()

# Transform the category_desc column
category_enc = pd.get_dummies(volunteer['native-country'])

# Take a look at the encoded columns
category_enc.head()

from sklearn.preprocessing import LabelEncoder

# Set up the LabelEncoder object
enc = LabelEncoder()

# Apply the encoding to the "Accessible" column
volunteer['Enc_Workclass'] = enc.fit_transform(volunteer['workclass'])

# Compare the two columns
print(volunteer[['workclass', 'Enc_Workclass']].head())
from sklearn.preprocessing import LabelEncoder

# Set up the LabelEncoder object
enc = LabelEncoder()

# Apply the encoding to the "Accessible" column
volunteer['Enc_Occ'] = enc.fit_transform(volunteer['occupation'])

# Compare the two columns
print(volunteer[['occupation', 'Enc_Occ']].head())

enc = LabelEncoder()

# Apply the encoding to the "Accessible" column
volunteer['Enc_Edu'] = enc.fit_transform(volunteer['education'])

# Compare the two columns
print(volunteer[['education', 'Enc_Edu']].head())

from sklearn.preprocessing import MinMaxScaler


list_of_features = ["fnlwgt","capital-gain","capital-loss"]

scaler = MinMaxScaler()

scaler.fit(volunteer[list_of_features])

scaled_data = scaler.transform(volunteer[list_of_features])

print(scaled_data.max())
print(scaled_data.min())

from sklearn.model_selection import train_test_split

list_of_features=['age','Enc_Workclass','fnlwgt','educational-num','Enc_Occ','capital-gain','capital-loss']
label_column_name=['income_>50K']
x_data = volunteer[list_of_features]
y_data = volunteer.iloc[:, -1]

x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.2, random_state=42)

from sklearn.neighbors import KNeighborsClassifier
scaler = MinMaxScaler()
scaler.fit(x_train)

X_train_scaled = scaler.transform(x_train)

X_test_scaled = scaler.transform(x_test)
knn=KNeighborsClassifier()

#train
knn.fit(X_train_scaled, y_train)

# scoring on the scaled test set
print("Scaled test set accuracy: {:.2f}".format(
    knn.score(X_test_scaled, y_test)))

"""Perform classification and calculate accuracy using logistic regression (Use sklearn library). Perform necessary pre-processing on the dataset before classification. Use 8:2 train-test split."""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
X= volunteer[list_of_features]
Y= volunteer.iloc[:, -1]
x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
#Train the model
model = LogisticRegression()
model.fit(x_train, y_train) #Training the model
predictions = model.predict(x_test)

print( accuracy_score(y_test, predictions))

"""Perform classification and calculate accuracy using decision tree (Use sklearn library). Perform necessary pre-processing on the dataset before classification. Use 8:2 train-test split."""

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
X= volunteer[list_of_features]
Y= volunteer.iloc[:, -1]
#X = animal.iloc[:,1:17]
#y = animal.iloc[:,17]
X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.2,random_state=1)
clf = DecisionTreeClassifier(criterion='entropy',random_state=1)
clf.fit(X_train,y_train)
y_pred = clf.predict(X_test)
score=accuracy_score(y_pred,y_test)
print(score)

"""Compare the accuracy and plot them as a bar chart using matplotlib/seaborn"""

import matplotlib.pyplot as plt

x = ['DecisionTreeClassifier', 'LogisticRegression']
y = [score, accuracy_score(y_test, predictions)]

plt.bar(x, y)
plt.show()

from sklearn.svm import SVC
svc = SVC(kernel="linear")
svc.fit(x_train, y_train)

print("Training accuracy of the model is {:.2f}".format(svc.score(x_train, y_train)))
print("Testing accuracy of the model is {:.2f}".format(svc.score(x_test, y_test)))

predictions = svc.predict(x_test)
print(predictions)
from sklearn.metrics import confusion_matrix
mat=confusion_matrix(predictions, y_test)
print(mat)

from seaborn import heatmap
heatmap(mat , cmap="Pastel1_r", xticklabels=['class_0' ,'class_1' ,'class_2'], yticklabels=['class_0' ,'class_1', 'class_2'], annot=True)

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=50)
rfc.fit(x_train, y_train)

print("The Training accuracy of the model is {:.2f}".format(rfc.score(x_train, y_train)))
print("The Testing accuracy of the model is {:.2f}".format(rfc.score(x_test, y_test)))
predictions = rfc.predict(x_test)
from sklearn.metrics import confusion_matrix
mat=confusion_matrix(predictions, y_test)
print(mat)

from seaborn import heatmap
heatmap(mat , cmap="Pastel1_r", xticklabels=['class_0' ,'class_1' ,'class_2'], yticklabels=['class_0' ,'class_1', 'class_2'], annot=True)

from sklearn.neural_network import MLPClassifier
nnc=MLPClassifier(hidden_layer_sizes=(7), activation="relu", max_iter=10000)
nnc.fit(x_train, y_train)

print("The Training accuracy of the model is {:.2f}".format(nnc.score(x_train, y_train)))
print("The Testing accuracy of the model is {:.2f}".format(nnc.score(x_test, y_test)))

predictions = nnc.predict(x_test)
print(predictions)

from sklearn.metrics import confusion_matrix
mat=confusion_matrix(predictions, y_test)
print(mat)

from seaborn import heatmap
heatmap(mat , cmap="Pastel1_r", xticklabels=['class_0' ,'class_1' ,'class_2'], yticklabels=['class_0' ,'class_1', 'class_2'], annot=True)

import matplotlib.pyplot as plt

x = ['SVC', 'RandomForestClassifier', 'MLPClassifier']
y = [score, accuracy_score(y_test, predictions)]

plt.bar(x, y)
plt.show()

from sklearn.decomposition import PCA 
pca = PCA(n_components=2)

principal_components= pca.fit_transform(cancer_data)
print(principal_components)

pca.explained_variance_ratio_

sum(pca.explained_variance_ratio_)

principal_df = pd.DataFrame(data=principal_components, columns=["principle component 1", "principle component 2"])
#principal_df.head()
main_df=pd.concat([principal_df, cancer[["target"]]], axis=1)

main_df.head()

fig = plt.figure(figsize = (8,8))
ax = fig.add_subplot(1,1, 1) 
ax.set_xlabel('Principal Component 1', fontsize = 15)
ax.set_ylabel('Principal Component 2', fontsize = 15)
ax.set_title('2 component PCA', fontsize = 20)
targets = [0, 1]
colors = ['r', 'g']
for target, color in zip(targets,colors):
    indicesToKeep = main_df['target'] == target
    #print(indicesToKeep)
    ax.scatter(main_df.loc[indicesToKeep, 'principle component 1']
               , main_df.loc[indicesToKeep, 'principle component 2']
               , c = color
               , s = 50)
ax.legend(["Malignant", "Benign"])
ax.grid()

X= main_df.drop("target" , axis=1)
y= main_df["target"]

x_train, x_test, y_train, y_test = train_test_split(X , y , test_size=0.2, random_state=42)

svc.fit(x_train, y_train)

print("Training accuracy of the model is {:.2f}".format(svc.score(x_train, y_train)))
print("Testing accuracy of the model is {:.2f}".format(svc.score(x_test, y_test)))

predictions = svc.predict(x_test)
print(predictions)

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=50)
rfc.fit(x_train, y_train)

print("The Training accuracy of the model is {:.2f}".format(rfc.score(x_train, y_train)))
print("The Testing accuracy of the model is {:.2f}".format(rfc.score(x_test, y_test)))

predictions = rfc.predict(x_test)

from sklearn.neural_network import MLPClassifier
nnc=MLPClassifier(hidden_layer_sizes=(7), activation="relu", max_iter=10000)

nnc.fit(x_train, y_train)

print("The Training accuracy of the model is {:.2f}".format(nnc.score(x_train, y_train)))
print("The Testing accuracy of the model is {:.2f}".format(nnc.score(x_test, y_test)))

predictions = nnc.predict(x_test)
print(predictions)

import matplotlib.pyplot as plt

x = ['SVC', 'RandomForestClassifier', 'MLPClassifier']
y = [score, accuracy_score(y_test, predictions)]

plt.bar(x, y)
plt.show()